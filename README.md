# Word-Vector-Models
The above jupyter-notebook contains implementations co-occurence matrix based models such as the naive and GLoVe models. In order to increase the understandability of the learnt vectors we approximate the learnt vectors in 2-D space using a generalization of Principal Component Analysin(PCA) which is Singular Value Decomposition(SVD). Here we select the top 2 principal componenets. The notebook shows how similar words form clusters even after such reductions. This helps in  analogies finding among words using the cosine rule of vectors. Questions such as <em>"if man is to a king then woman is to a?"</em> can be answered using these rules. 

