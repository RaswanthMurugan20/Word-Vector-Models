# Word-Vector-Models
The above jupyter-notebook contains implementations co-occurence matrix based models such as the naive and GLoVe models. In order to increase the understandability of the learnt vectors we approximate the learnt vectors in 2-D space using a generalization of Principal Component Analysin(PCA) which is Singular Value Decomposition(SVD). Here we select the top 2 principal componenets. The notebook shows how similar words form clusters even after such reductions. The notebook uses the genism library that helps in  finding relations among word vectors. Questions related to analogies between words such as <em>"if man is to a king then woman is to a?"</em> can be answered using the cosine rule of vectors. Such analogies can also be used as a sanity check for learnt word vectors and important part of NLP.  

